{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Kx55PQTfgB"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 4\n",
        "\n",
        "The objective of this assignment is to introduce you to federated learning, with a particular focus on the challenges posed by data heterogeneity. Specifically, you will explore well-known approaches for addressing statistical heterogeneity, approaching the problem from multiple angles.\n",
        "\n",
        "You have been provided with a manual that includes detailed descriptions of each task. Follow the instructions in the manual carefully, and add your answers and implementations directly in this notebook.\n",
        "\n",
        "Note: In this assignment, we concentrate on one common type of statistical heterogeneity: label skew. While feature skew is another significant aspect of heterogeneity, it will not be the focus of this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5RdSHuTTWhU",
        "outputId": "b10b87cb-41c5-47d0-b27a-4a60b3f4c7a1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "from collections import defaultdict\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id4kAraZa3bv",
        "outputId": "5966ddac-f13c-4d90-eaf1-882084bb9e1f"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Calculate sizes for training and testing datasets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Define fixed indices for train and test splits\n",
        "train_indices = list(range(train_size))\n",
        "test_indices = list(range(train_size, len(dataset)))\n",
        "\n",
        "# Create Subsets based on these indices\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "train_images = dataset.data[train_dataset.indices].numpy()\n",
        "train_labels = dataset.targets[train_dataset.indices].numpy()\n",
        "test_images = dataset.data[test_dataset.indices].numpy()\n",
        "test_labels = dataset.targets[test_dataset.indices].numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def partition_data(train_data, train_targets, test_data, test_targets, num_clients, dirichlet_alpha=0.5, batch_size=32):\n",
        "    num_classes = len(np.unique(train_targets))\n",
        "    client_data_indices = defaultdict(list)\n",
        "    client_test_indices = defaultdict(list)\n",
        "\n",
        "    # Partition Train Data and Capture Proportions\n",
        "    client_proportions = []\n",
        "    for c in range(num_classes):\n",
        "        class_indices = np.where(train_targets == c)[0]\n",
        "        proportions = np.random.dirichlet([dirichlet_alpha] * num_clients)\n",
        "        client_proportions.append(proportions)\n",
        "\n",
        "        # Calculate split sizes and assign indices to each client\n",
        "        split_sizes = (np.cumsum(proportions) * len(class_indices)).astype(int)[:-1]\n",
        "        client_indices_split = np.split(class_indices, split_sizes)\n",
        "\n",
        "        for i, indices in enumerate(client_indices_split):\n",
        "            client_data_indices[i].extend(indices)\n",
        "\n",
        "    # Partition Test Data Using the Same Proportions as Train Data\n",
        "    for c, proportions in enumerate(client_proportions):\n",
        "        class_indices = np.where(test_targets == c)[0]\n",
        "        split_sizes = (np.cumsum(proportions) * len(class_indices)).astype(int)[:-1]\n",
        "        client_indices_split = np.split(class_indices, split_sizes)\n",
        "\n",
        "        for i, indices in enumerate(client_indices_split):\n",
        "            client_test_indices[i].extend(indices)\n",
        "\n",
        "    # Create Federated Loaders\n",
        "    federated_train_loaders, federated_test_loaders = [], []\n",
        "    for i in range(num_clients):\n",
        "        client_train_data = torch.tensor(train_data[client_data_indices[i]], dtype=torch.float32).unsqueeze(1)\n",
        "        client_train_targets = torch.tensor(train_targets[client_data_indices[i]], dtype=torch.long)\n",
        "        train_loader = DataLoader(TensorDataset(client_train_data, client_train_targets), batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "        federated_train_loaders.append(train_loader)\n",
        "\n",
        "        client_test_data = torch.tensor(test_data[client_test_indices[i]], dtype=torch.float32).unsqueeze(1)\n",
        "        client_test_targets = torch.tensor(test_targets[client_test_indices[i]], dtype=torch.long)\n",
        "        test_loader = DataLoader(TensorDataset(client_test_data, client_test_targets), batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "        federated_test_loaders.append(test_loader)\n",
        "\n",
        "    # Create Centralized Loader by combining data from federated loaders\n",
        "    centralized_data, centralized_targets = [], []\n",
        "    for train_loader in federated_train_loaders:\n",
        "        for images, targets in train_loader:\n",
        "            centralized_data.append(images)\n",
        "            centralized_targets.append(targets)\n",
        "    \n",
        "    centralized_data = torch.cat(centralized_data)\n",
        "    centralized_targets = torch.cat(centralized_targets)\n",
        "    train_centralized_loader = DataLoader(TensorDataset(centralized_data, centralized_targets), batch_size=batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "    centralized_test_data = torch.tensor(test_data, dtype=torch.float32).unsqueeze(1)\n",
        "    centralized_test_targets = torch.tensor(test_targets, dtype=torch.long)\n",
        "    test_centralized_loader = DataLoader(TensorDataset(centralized_test_data, centralized_test_targets), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return federated_train_loaders, federated_test_loaders, train_centralized_loader, test_centralized_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ygdrwc1kjJ87"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 500)\n",
        "        self.fc2 = nn.Linear(500,10)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LESW3bG3Vd0m"
      },
      "source": [
        "# Task 1: Centralized Vs Federated Scenario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this task, you will analyze the theoretical equivalence between FedSGD and a centralized training scenario. The code for both implementations has been provided. To minimize differences caused by floating-point precision, a batch size of 1 is used, along with full-batch gradient descent. You may change the device to cpu if you wish, as gpu will not be utilized for batch size of 1.\n",
        "\n",
        "Since we are working with batch size 1 and full-batch gradient descent, accuracy is not the focus here. Instead, you will monitor gradient divergence by tracking the sum or magnitude of the gradients at each round and observe how they evolve in both settings.\n",
        "\n",
        "Theoretically, FedSGD and centralized training should yield equivalent results. However, in this task, you may observe discrepancies. Your objective is to understand the code and identify the error that causes this divergence. You may want to review why FedSGD and full-batch centralized gradient descent are theoretically equivalent by revisiting the gradient descent steps on the global objective, as covered in class. Finally, assess whether the code aligns with these theoretical steps. If you believe they should not be equivalent, provide a reasoned justification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_clients = 5\n",
        "dirichlet_alpha = 0.8\n",
        "batch_size = 1\n",
        "\n",
        "# Partition data and create loaders\n",
        "federated_train_loaders, federated_test_loaders, train_centralized_loader, test_centralized_loader = partition_data(\n",
        "    train_images / 255.0, train_labels, test_images / 255.0, test_labels, num_clients, dirichlet_alpha, batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "bDOOZ9cwWNr_"
      },
      "outputs": [],
      "source": [
        "def fedsgd_training(global_model, train_loaders, num_clients, rounds, lr):\n",
        "    round_avg_updates = []\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    global_model.to(device)\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    client_data_counts = [len(train_loader.dataset) for train_loader in train_loaders]\n",
        "    total_data_count = 0\n",
        "    for count in client_data_counts:\n",
        "        total_data_count += count\n",
        "\n",
        "    for round_num in range(rounds):\n",
        "        client_updates = []\n",
        "        print(f\"Round {round_num + 1}/{rounds}\")\n",
        "\n",
        "        for client_id in range(num_clients):\n",
        "            overall_update = local_train(global_model, train_loaders[client_id], lr)\n",
        "            client_update_device = {name: update.to(device) for name, update in overall_update.items()}\n",
        "            client_updates.append(client_update_device)\n",
        "\n",
        "            client_update_sum = 0\n",
        "            for update in client_update_device.values():\n",
        "                client_update_sum += update.abs().sum().item()\n",
        "            print(f\"Client {client_id + 1} Update Sum at Round {round_num + 1}: {client_update_sum}\")\n",
        "\n",
        "        avg_update = {}\n",
        "        for name in global_model.state_dict():\n",
        "            grad_sum = torch.zeros_like(global_model.state_dict()[name], device=device)\n",
        "            for client_id in range(num_clients):\n",
        "                grad_sum += client_updates[client_id][name]\n",
        "            avg_update[name] = grad_sum\n",
        "\n",
        "        round_avg_updates.append(avg_update)\n",
        "\n",
        "        avg_update_sum = 0\n",
        "        for update in avg_update.values():\n",
        "            avg_update_sum += update.abs().sum().item()\n",
        "        print(f\"Averaged Update Sum at Round {round_num + 1}: {avg_update_sum}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for name, param in global_model.named_parameters():\n",
        "                param -= avg_update[name]\n",
        "\n",
        "    return round_avg_updates\n",
        "\n",
        "\n",
        "\n",
        "def local_train(model, train_loader, lr):\n",
        "    local_model = SimpleCNN()\n",
        "    local_model.load_state_dict(model.state_dict())\n",
        "    local_model.train()\n",
        "\n",
        "    optimizer = optim.SGD(local_model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    local_model.to(device)\n",
        "\n",
        "    initial_weights = {name: param.clone() for name, param in local_model.named_parameters()}\n",
        "\n",
        "    for epoch in range(1):\n",
        "        print(f\"Epoch {epoch + 1}/{1} for local client\")\n",
        "        optimizer.zero_grad() \n",
        "        batch_iterator = tqdm(train_loader, desc=f\"Training Batches (Epoch {epoch + 1})\")\n",
        "\n",
        "        # Loop over batches\n",
        "        for x_train, y_train in batch_iterator:\n",
        "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "            outputs = local_model(x_train)\n",
        "            loss = criterion(outputs, y_train)\n",
        "            loss.backward()\n",
        "            batch_iterator.set_postfix(loss=loss.item())\n",
        "\n",
        "        optimizer.step()  \n",
        "\n",
        "    overall_update = {name: initial_weights[name] - param for name, param in local_model.named_parameters()}\n",
        "    \n",
        "    return overall_update\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "-TFTDRziVBj9"
      },
      "outputs": [],
      "source": [
        "def centralized_training_updates(global_model, train_loader, epochs, lr):\n",
        "    centralized_model = SimpleCNN()\n",
        "    centralized_model.load_state_dict(global_model.state_dict())\n",
        "    centralized_model.train()\n",
        "\n",
        "    optimizer = optim.SGD(centralized_model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    centralized_model.to(device)\n",
        "\n",
        "    epoch_updates = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        epoch_initial_weights = {name: param.clone() for name, param in centralized_model.named_parameters()}\n",
        "        optimizer.zero_grad()\n",
        "        batch_iterator = tqdm(train_loader, desc=f\"Training Batches (Epoch {epoch + 1})\")\n",
        "\n",
        "        for x_train, y_train in batch_iterator:\n",
        "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
        "            outputs = centralized_model(x_train)\n",
        "            loss = criterion(outputs, y_train)\n",
        "            loss.backward()\n",
        "\n",
        "            batch_iterator.set_postfix(loss=loss.item())\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_update = {name: param - epoch_initial_weights[name]  for name, param in centralized_model.named_parameters()}\n",
        "        epoch_updates.append(epoch_update)\n",
        "\n",
        "        epoch_update_sum = 0\n",
        "        for update in epoch_update.values():\n",
        "            epoch_update_sum += update.abs().sum().item()\n",
        "            \n",
        "        print(f\"Epoch {epoch + 1} Update Sum: {epoch_update_sum}\")\n",
        "\n",
        "    global_model.load_state_dict(centralized_model.state_dict())\n",
        "\n",
        "    return epoch_updates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "_knb-8et2-JX"
      },
      "outputs": [],
      "source": [
        "# Initialize model and parameters\n",
        "rounds = 4\n",
        "lr = 1e-4\n",
        "initial_model = SimpleCNN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLIlOCVhFlYm",
        "outputId": "2680c72c-7c30-4e22-f7eb-4cb38593d5bc"
      },
      "outputs": [],
      "source": [
        "# Centralized Training\n",
        "global_model_centralized = SimpleCNN()\n",
        "global_model_centralized.load_state_dict(initial_model.state_dict())\n",
        "centralized_updates = centralized_training_updates(global_model_centralized, train_centralized_loader, epochs=rounds, lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8miFVteUm53o",
        "outputId": "bbc8426b-dec3-49e3-ebea-d334a6ea4275"
      },
      "outputs": [],
      "source": [
        "# Fedsgd\n",
        "global_model_fedsgd = SimpleCNN()\n",
        "global_model_fedsgd.load_state_dict(initial_model.state_dict())\n",
        "fedsgd_avg_updates = fedsgd_training(global_model_fedsgd, federated_train_loaders, num_clients, rounds=rounds, lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "pEFk4VyQyuRL",
        "outputId": "4b38412f-e163-4991-9c23-022098c27aaf"
      },
      "outputs": [],
      "source": [
        "# Calculate update magnitudes for each round\n",
        "centralized_magnitudes = [sum(param.norm().item() for param in round_update.values()) for round_update in centralized_updates]\n",
        "fedsgd_magnitudes = [sum(param.norm().item() for param in update.values()) for update in fedsgd_avg_updates]\n",
        "\n",
        "# Plot update magnitudes\n",
        "x_labels = range(1, rounds + 1)\n",
        "width = 0.25\n",
        "\n",
        "plt.bar([x - width for x in x_labels], centralized_magnitudes, width=width, label=\"Centralized\")\n",
        "plt.bar(x_labels, fedsgd_magnitudes, width=width, label=\"FedSGD\")\n",
        "plt.xlabel(\"Round\")\n",
        "plt.ylabel(\"Average Update Magnitude\")\n",
        "plt.title(\"Update Magnitude Comparison Across Methods\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Check if the gradients are approximately the same\n",
        "tolerance = 1e-2\n",
        "success = all(abs(c - f) <= tolerance for c, f in zip(centralized_magnitudes, fedsgd_magnitudes))\n",
        "\n",
        "if success:\n",
        "    print(\"Success! The gradients are approximately the same.\")\n",
        "else:\n",
        "    print(\"Keep trying\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: FedAvg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement the fedavg_training function to perform Federated Averaging (FedAvg) over multiple communication rounds. Each round, collect updates from clients by training locally on their data and average these updates to form a new global model.\n",
        "\n",
        "Additionally, implement the local_train function to handle client-side training. You are not restricted to using full-batch gradient descent; feel free to use mini-batch gradient descent instead. Make sure your implementation aligns with the provided code to plot the accuracies, as you should not modify that code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(global_model, test_loaders):\n",
        "    global_model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    global_model.to(device)\n",
        "\n",
        "    client_accuracies = []\n",
        "    total_correct, total_samples = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for client_id, test_loader in enumerate(test_loaders):\n",
        "            client_correct, client_total = 0, 0\n",
        "            print(f\"\\nEvaluating Client {client_id}\")\n",
        "\n",
        "            for images, labels in test_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = global_model(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "                client_correct += (predicted == labels).sum().item()\n",
        "                client_total += labels.size(0)\n",
        "\n",
        "            client_accuracy = (client_correct / client_total) * 100\n",
        "            client_accuracies.append(client_accuracy)\n",
        "\n",
        "            print(f\"Client {client_id} Accuracy: {client_accuracy:.2f}%\")\n",
        "\n",
        "            total_correct += client_correct\n",
        "            total_samples += client_total\n",
        "\n",
        "    total_accuracy = (total_correct / total_samples) * 100\n",
        "    print(f\"Total Accuracy: {total_accuracy:.2f}%\")\n",
        "    return client_accuracies, total_accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fedavg_training(global_model, train_loaders, num_clients, rounds, epochs, lr, test_loaders):\n",
        "    round_avg_updates = []\n",
        "    round_accuracies = [] \n",
        "    # To Do\n",
        "    return round_avg_updates, round_accuracies\n",
        "\n",
        "\n",
        "\n",
        "def local_train(model, train_loader, epochs, lr):\n",
        "    overall_update = {name: torch.zeros_like(param) for name, param in model.state_dict().items()}\n",
        "    #To Do\n",
        "    return overall_update\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code you may change the learning rate or the batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "gMiV_ZN5m9hn",
        "outputId": "6bb76ad3-4ab8-4946-9f00-560b7660765c"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_clients = 5\n",
        "batch_size = 128\n",
        "rounds = 3\n",
        "epochs = 20\n",
        "lr = 0.001\n",
        "dirichlet_alphas = [2, 0.5, 0.1]\n",
        "\n",
        "def run_fedavg(alpha):\n",
        "    train_loaders, test_loaders,_,_ = partition_data(train_images / 255, train_labels, test_images / 255, test_labels, num_clients, alpha, batch_size)\n",
        "    global_model_fedavg = SimpleCNN()\n",
        "    _, round_accuracies = fedavg_training(global_model_fedavg, train_loaders, num_clients, rounds=rounds, epochs=epochs, lr=lr, test_loaders=test_loaders)\n",
        "    return round_accuracies\n",
        "\n",
        "accuracies_per_alpha = []\n",
        "\n",
        "for alpha in dirichlet_alphas:\n",
        "    round_accuracies = run_fedavg(alpha)\n",
        "    accuracies_per_alpha.append(round_accuracies)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting accuracy per round for each alpha value\n",
        "plt.figure()\n",
        "for idx, alpha in enumerate(dirichlet_alphas):\n",
        "    plt.plot(range(1, rounds + 1), accuracies_per_alpha[idx], marker='o', label=f'Alpha = {alpha}')\n",
        "\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.title('FedAvg Accuracy per Round across Different Dirichlet Alphas')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJz1KoNdVido"
      },
      "source": [
        "# Task 3: Scaffold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this task, you will implement the SCAFFOLD algorithm, which addresses client heterogeneity through variance reduction. We will evaluate the implementation in a 0.1 Dirichlet heterogeneous setting, and accuracy plotting code is provided.\n",
        "\n",
        "The implementation of SCAFFOLD is similar to FedAvg, with only a few additional steps at both the server and client side. For reference, you may consult the SCAFFOLD paper to understand these adjustments in detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "u9t0tpiL7APb"
      },
      "outputs": [],
      "source": [
        "def local_train_scaffold(model, train_loader, c_global, c_local, epochs, lr):\n",
        "    overall_update = {name: torch.zeros_like(param) for name, param in model.state_dict().items()}\n",
        "    new_c_local = {name: torch.zeros_like(param) for name, param in model.state_dict().items()}\n",
        "    # To Do\n",
        "    return overall_update, new_c_local\n",
        "\n",
        "def federated_scaffold(global_model, train_loaders, test_loaders, num_clients, rounds, epochs, local_lr):\n",
        "    round_accuracies = []\n",
        "    # To Do\n",
        "    return round_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRVOEp13Udtf"
      },
      "outputs": [],
      "source": [
        "num_clients = 5\n",
        "rounds = 3\n",
        "epochs = 20\n",
        "lr = 0.01\n",
        "mu = 0.001\n",
        "\n",
        "# Run federated SCAFFOLD with 0.1 Dirichlet and plot accuracy per round\n",
        "def run_scaffold():\n",
        "    # Partition data with a Dirichlet alpha of 0.1\n",
        "    train_loaders, test_loaders,_,_ = partition_data(train_images / 255, train_labels, test_images / 255, test_labels, num_clients, 0.1, batch_size)\n",
        "    global_model_scaffold = SimpleCNN()\n",
        "    round_accuracies = federated_scaffold(global_model_scaffold, train_loaders, test_loaders, num_clients, rounds, epochs, local_lr=lr)\n",
        "    return round_accuracies\n",
        "\n",
        "# Run SCAFFOLD and get accuracy per round\n",
        "scaffold_accuracies = run_scaffold()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(range(1, rounds + 1), scaffold_accuracies, marker='o', label='SCAFFOLD Alpha = 0.1')\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('SCAFFOLD Accuracy per Round (Dirichlet Alpha = 0.1)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 4: Gradient Harmonization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this task, you will examine how heterogeneity influences the gradient updates sent by each client. Specifically, you will analyze and quantify gradient conflicts at varying levels of heterogeneity.\n",
        "\n",
        "First, implement the FedAvg code (You can reuse your task 2 implementation) and integrate the provided conflict-counting function at the end of each round to measure the number of conflicts in different heterogeneous settings.\n",
        "\n",
        "Next, implement the Gradient Harmonization algorithm to harmonize conflicting gradients.\n",
        "\n",
        "Finally, evaluate the performance of the harmonized gradients under a 0.1 Dirichlet scenario, and compare the number of conflicts before and after harmonization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import builtins\n",
        "sum = builtins.sum\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return torch.dot(vec1, vec2) / (torch.norm(vec1) * torch.norm(vec2))\n",
        "\n",
        "def gradient_conflict_counter(client_updates):\n",
        "    total_conflicts = 0  # Variable to store the total conflicts across all layers\n",
        "    num_clients = len(client_updates)\n",
        "\n",
        "    # Iterate over each layer (key) in the client updates\n",
        "    for layer_name in client_updates[0].keys():\n",
        "        # Collect vectors for this layer across all clients\n",
        "        layer_vectors = [client_update[layer_name].view(-1).cpu() for client_update in client_updates]\n",
        "\n",
        "        # Check pairwise cosine similarity for this layer and count conflicts\n",
        "        for i in range(num_clients):\n",
        "            for j in range(i + 1, num_clients):\n",
        "                sim = cosine_similarity(layer_vectors[i], layer_vectors[j])\n",
        "                if sim < 0:\n",
        "                    total_conflicts += 1\n",
        "\n",
        "    print(f\"Total gradient conflicts detected: {total_conflicts}\")\n",
        "    return total_conflicts\n",
        "\n",
        "def Federated_gradient_conflict(global_model, train_loaders, num_clients, rounds, epochs, lr, test_loaders):\n",
        "    round_avg_updates = []\n",
        "    round_accuracies = []  \n",
        "    conflict_counts = [] \n",
        "    # To Do\n",
        "    return round_avg_updates, round_accuracies, conflict_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_clients = 5\n",
        "batch_size = 128\n",
        "rounds = 3\n",
        "epochs = 20\n",
        "lr = 0.001\n",
        "dirichlet_alphas = [2, 0.5, 0.1]\n",
        "\n",
        "def run_fedconflict(alpha):\n",
        "    train_loaders, test_loaders,_,_ = partition_data(train_images / 255, train_labels, test_images / 255, test_labels, num_clients, alpha, batch_size)\n",
        "    global_model_fedavg = SimpleCNN()\n",
        "    _, round_accuracies,conflict_counts = Federated_gradient_conflict(global_model_fedavg, train_loaders, num_clients, rounds=rounds, epochs=epochs, lr=lr, test_loaders=test_loaders)\n",
        "    return round_accuracies, conflict_counts\n",
        "\n",
        "accuracies_per_alpha = []\n",
        "conflicts_per_alpha = []\n",
        "\n",
        "for alpha in dirichlet_alphas:\n",
        "    round_accuracies, conflict_counts = run_fedconflict(alpha)\n",
        "    accuracies_per_alpha.append(round_accuracies)\n",
        "    conflicts_per_alpha.append(conflict_counts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for i, alpha in enumerate(dirichlet_alphas):\n",
        "    plt.plot(range(1, rounds + 1), conflicts_per_alpha[i], marker='o', label=f'Alpha = {alpha}')\n",
        "\n",
        "plt.xlabel(\"Rounds\")\n",
        "plt.ylabel(\"Number of Gradient Conflicts\")\n",
        "plt.title(\"Gradient Conflicts Across Rounds for Different Heterogeneity Levels (Dirichlet Alphas)\")\n",
        "plt.legend(title=\"Heterogeneity Level (Alpha)\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Hejx1UbUtuH"
      },
      "outputs": [],
      "source": [
        "def gradient_harmonization(client_updates):\n",
        "    # To Do\n",
        "    return client_updates\n",
        "\n",
        "def Federated_gradient_harmonization(global_model, train_loaders, num_clients, rounds, epochs, lr, test_loaders):\n",
        "    round_accuracies = []  # List to store total accuracy after each round\n",
        "    pre_harmonization_conflicts = []  # List to store conflicts before harmonization\n",
        "    post_harmonization_conflicts = []  # List to store conflicts after harmonization\n",
        "    # To Do\n",
        "    return round_accuracies, pre_harmonization_conflicts, post_harmonization_conflicts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_clients = 5\n",
        "batch_size = 128\n",
        "rounds = 3\n",
        "epochs = 20\n",
        "lr = 0.001\n",
        "alpha = 0.1\n",
        "\n",
        "# Run federated gradient harmonization\n",
        "train_loaders, test_loaders, _, _ = partition_data(\n",
        "    train_images / 255, train_labels, test_images / 255, test_labels, num_clients, alpha, batch_size\n",
        ")\n",
        "global_model_fedavg = SimpleCNN()\n",
        "round_accuracies, pre_harmonization_conflicts, post_harmonization_conflicts = Federated_gradient_harmonization(\n",
        "    global_model_fedavg, train_loaders, num_clients, rounds=rounds, epochs=epochs, lr=lr, test_loaders=test_loaders\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, rounds + 1), pre_harmonization_conflicts, marker='o', linestyle='-', label='Conflicts Before Harmonization')\n",
        "plt.plot(range(1, rounds + 1), post_harmonization_conflicts, marker='x', linestyle='--', label='Conflicts After Harmonization')\n",
        "plt.xlabel(\"Rounds\")\n",
        "plt.ylabel(\"Number of Gradient Conflicts\")\n",
        "plt.title(\"Gradient Conflicts Across Rounds (Alpha = 0.1)\")\n",
        "plt.legend()\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, rounds + 1), round_accuracies, marker='s', color='b')\n",
        "plt.xlabel(\"Rounds\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Model Accuracy Across Rounds (Alpha = 0.1)\")\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 5: FedSam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this task, you will explore Sharpness-Aware Minimization (SAM) in a federated learning setting by implementing the FedSAM algorithm. As with previous tasks, evaluate its performance in a 0.1 Dirichlet heterogeneous scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_sam_perturbation(local_model, criterion, x_train, y_train, rho):\n",
        "    # To Do\n",
        "    pass\n",
        "\n",
        "def local_train_sam(model, train_loader, epochs, lr, rho=0.0001):\n",
        "    overall_update = {name: torch.zeros_like(param) for name, param in model.state_dict().items()}\n",
        "    # To do\n",
        "    return overall_update\n",
        "\n",
        "def federated_training_sam(global_model, train_loaders, test_loaders, num_clients, rounds, epochs, local_lr):\n",
        "    round_accuracies = []\n",
        "    # To Do\n",
        "    return round_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the FedSAM experiment\n",
        "num_clients = 5\n",
        "rounds = 3\n",
        "epochs = 20\n",
        "lr = 0.001\n",
        "\n",
        "def run_fedsam():\n",
        "    # Partition data with a Dirichlet alpha of 0.1\n",
        "    train_loaders, test_loaders, _, _ = partition_data(train_images / 255, train_labels, test_images / 255, test_labels, num_clients, 0.1, batch_size)\n",
        "    global_model_sam = SimpleCNN()\n",
        "    round_accuracies = federated_training_sam(global_model_sam, train_loaders, test_loaders, num_clients, rounds, epochs, local_lr=lr)\n",
        "    return round_accuracies\n",
        "\n",
        "# Run FedSAM and get accuracy per round\n",
        "sam_accuracies = run_fedsam()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plotting accuracy per round for FedSAM with Dirichlet alpha 0.1\n",
        "plt.figure()\n",
        "plt.plot(range(1, rounds + 1), sam_accuracies, marker='o', label='FedSAM Alpha = 0.1')\n",
        "plt.xlabel('Rounds')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('FedSAM Accuracy per Round (Dirichlet Alpha = 0.1)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

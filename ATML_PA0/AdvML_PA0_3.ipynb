{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 0 Part 3 - CLIP and Zero-Shot Image Classification\n",
    "\n",
    "In our previous work, we focused solely on visual information, leveraging convolutional neural networks (CNNs) and Vision Transformers to analyze and interpret images. However, the field of machine learning has expanded to include multimodal models that integrate different types of data. This brings us to CLIP (Contrastive Language-Image Pretraining), a groundbreaking model that combines both text and images to learn their interrelationships.\n",
    "\n",
    "CLIP, developed by OpenAI, is designed to understand and connect textual descriptions with corresponding images. Unlike traditional models that operate within a single modality, CLIP learns to map both text and images into a shared feature space. This shared space enables the model to grasp the relationship between visual content and textual descriptions, making it possible to perform tasks that involve both types of information. For example, CLIP can interpret a sentence and find the image that best matches this description or generate a textual description for a given image.\n",
    "\n",
    "The ability to learn representations across different modalities opens up new possibilities for tasks that CLIP wasn't explicitly trained for. One of the most intriguing applications is zero-shot classification, where CLIP can classify images into categories it has never seen during training, simply based on the similarity of the image to textual descriptions of those categories. Another powerful feature is the ability to find the closest image match to a query caption, which can be particularly useful for image retrieval and recommendation systems.\n",
    "\n",
    "By implementing CLIP, we move beyond single-modality tasks and explore the potential of models that understand and integrate multiple forms of information. This approach not only enhances the capabilities of our models but also expands the range of applications they can address, making them more versatile and powerful in handling complex real-world tasks.\n",
    "\n",
    "Here is a list of resources to help you with this part:\n",
    "- [CLIP — Intuitively and Exhaustively Explained](https://medium.com/towards-data-science/clip-intuitively-and-exhaustively-explained-1d02c07dbf40) - A good article to get an overview of CLIP.\n",
    "- [The Annotated CLIP](https://amaarora.github.io/posts/2023-03-06_Understanding_CLIP.html) - A two-part series on a more implementation-focused overview of CLIP.\n",
    "- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) - The official CLIP paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Callable, Optional\n",
    "from dataclasses import dataclass\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import timm\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0. Getting our Data\n",
    "\n",
    "To implement CLIP effectively, we need a dataset that pairs complete sentences with their corresponding images, enabling the model to learn the relationship between textual descriptions and visual content. One such dataset is Flick8k, which contains thousands of images, each paired with multiple descriptive captions. This dataset is a good fit for training models that require understanding and associating both text and images.\n",
    "\n",
    "You can download Flick8k using [this link from Kaggle](https://www.kaggle.com/datasets/adityajn105/flickr8k). \n",
    "\n",
    "However, it's important to note that replicating the exact training process described in the original CLIP paper would require substantial computational resources. Instead, we can use a subset of the dataset for our experiments, which will allow us to explore CLIP's capabilities without the need for extensive training.\n",
    "\n",
    "In the CLIP architecture, we need to process both text and images through separate encoders. The Text Encoder converts input captions into embeddings, while the Image Encoder transforms images into a compatible feature space. One critical step in working with text is tokenization. Tokenization involves breaking down a sentence into smaller units, such as words or subwords, which are then converted into numerical representations that the model can understand.\n",
    "\n",
    "Tokenization prepares the text for embedding by splitting it into manageable pieces and mapping these pieces to unique identifiers. For example, the sentence \"A cat sitting on a mat\" might be tokenized into individual words or subword units, each of which is then converted into an embedding vector by the Text Encoder. This process allows the model to handle various lengths and structures of text inputs effectively.\n",
    "\n",
    "By incorporating both text and image encoders into our implementation, and understanding the importance of tokenization, we can create a system that learns to relate textual descriptions to visual content accurately. This approach enables the model to perform tasks such as matching captions to images and understanding the semantic connections between different modalities.\n",
    "\n",
    "In the cell(s) below, a configuration class has been defined for the hyperparameters of the model, and a `CLIPDataset` class to make working with the data less of a hassle. Take some time to understand how the tokenizer works and what one batch of data would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    images_path: str = \"flickr8k/Images\"\n",
    "    captions_path: str = \"flickr8k/captions.txt\"\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 3\n",
    "\n",
    "    image_encoder: str = 'resnet50'\n",
    "    image_emb_size: int = 2048\n",
    "    image_size: int = 224\n",
    "    image_encoder_lr: float = 1e-4\n",
    "    \n",
    "    text_encoder: str = 'distilbert-base-uncased'\n",
    "    text_embedding: str = 768\n",
    "    text_tokenizer: str = 'distilbert-base-uncased'\n",
    "    max_length: int = 200\n",
    "    text_encoder_lr: float = 1e-5\n",
    "\n",
    "    projection_dim: int = 256\n",
    "    head_lr: float = 1e-3\n",
    "    weight_decay: float = 1e-3\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        image_filenames: List[str], \n",
    "        captions: List[str], \n",
    "        tokenizer: Callable[[List[str]], Dict[str, torch.Tensor]], \n",
    "        transform: Optional[Callable[[Image.Image], Image.Image]] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with image filenames, captions, a tokenizer function, and optional image transforms.\n",
    "        \n",
    "        :param image_filenames: List of image file names.\n",
    "        :param captions: List of captions corresponding to the images.\n",
    "        :param tokenizer: Function to tokenize captions. It should return a dictionary with tensors.\n",
    "        :param transform: Optional transform to be applied on images.\n",
    "        \"\"\"\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = captions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "        # Tokenize all captions\n",
    "        self.encoded_captions = tokenizer(captions, padding=True, truncation=True, max_length=cfg.max_length)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieves an item from the dataset given an index.\n",
    "\n",
    "        :param idx: Index of the item to retrieve.\n",
    "        :return: Dictionary with 'image' and 'caption' keys.\n",
    "        \"\"\"\n",
    "        # Get encoded caption\n",
    "        encoded_caption = {key: torch.tensor(values[idx]) for key, values in self.encoded_captions.items()}\n",
    "        \n",
    "        # Load and process image\n",
    "        image_path = f\"{cfg.images_path}/{self.image_filenames[idx]}\"\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Return dictionary with tensors\n",
    "        return {\n",
    "            'image': image,\n",
    "            'caption': encoded_caption\n",
    "        }\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of items in the dataset.\n",
    "\n",
    "        :return: Number of items.\n",
    "        \"\"\"\n",
    "        return len(self.captions)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(cfg.captions_path)\n",
    "print(f\"Size of original dataset: {df.shape}\")\n",
    "\n",
    "# Remove rows of duplicate images (otherwise we have one image with multiple captions)\n",
    "df = df.drop_duplicates(subset='image', keep='first')\n",
    "print(f\"Size after deduplication: {df.shape}\")\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(cfg.text_tokenizer)\n",
    "\n",
    "clip_ds = CLIPDataset(\n",
    "    image_filenames=df.image.values.tolist(),\n",
    "    captions=df.caption.values.tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((cfg.image_size, cfg.image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    ")\n",
    "clip_dl = DataLoader(\n",
    "    clip_ds,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Image Encoder\n",
    "\n",
    "One of the core components of CLIP is the Image Encoder, which transforms images into embeddings that are compared against text embeddings. This process allows CLIP to map visual information into the same feature space as textual descriptions, enabling effective cross-modal comparisons.\n",
    "\n",
    "For our implementation, we will use the `timm` library to load a pre-trained ResNet50 model. ResNet50 is a popular convolutional neural network known for its robust feature extraction capabilities. However, instead of using the full model with its classifier head, we'll focus on the feature extractor backbone. This allows us to leverage ResNet50's powerful feature extraction while bypassing the classification layer, which is unnecessary for our purpose.\n",
    "\n",
    "Take some time to go through the documentation for `timm` to load in the feature extractor for a ResNet50.\n",
    "\n",
    "To enhance training efficiency, consider freezing the parameters of the Image Encoder. Since the focus is on training the projection heads for the CLIP model, freezing the Image Encoder parameters can prevent unnecessary updates and speed up the training process. This approach allows the model to concentrate on learning how to project the image embeddings into the shared feature space.\n",
    "\n",
    "When performing a forward pass through the Image Encoder, it's important to note the dimensionality of the output features. We are aiming for a tensor of shape $(B, d)$, so if the tensor has a higher dimensionality, consider performing some form of global pooling.\n",
    "\n",
    "By implementing and fine-tuning the Image Encoder in this way, we ensure that our model effectively converts images into a form that can be used in conjunction with text embeddings, facilitating accurate cross-modal comparisons and achieving the desired functionalities of CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 model_name: str,\n",
    "                 trainable: bool = False):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, x: torch.tensor):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = next(iter(clip_dl))['image']\n",
    "print(img_batch.shape)\n",
    "\n",
    "img_encoder = ImageEncoder(cfg.image_encoder)\n",
    "img_enc_out = img_encoder(img_batch)\n",
    "\n",
    "print(img_enc_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Text Encoder\n",
    "\n",
    "The Text Encoder in CLIP is responsible for transforming textual descriptions into embeddings that can be compared with image embeddings. For this purpose, we will use DistilBERT, a lighter and faster variant of the BERT model. DistilBERT retains most of BERT’s language understanding capabilities while reducing its size and computational requirements, making it well-suited for our task.\n",
    "\n",
    "**Tokenization** is a crucial step in processing text for neural network models. It involves breaking down text into smaller units, such as words or subwords, and converting these units into numerical representations that the model can understand. For example, the sentence \"A cat sitting on a mat\" might be tokenized into individual words or subword units.\n",
    "\n",
    "In the context of DistilBERT, tokenization is performed using a tokenizer that splits text into tokens and maps them to corresponding IDs from a vocabulary. The tokenizer also handles special tokens required by the model, such as the [CLS] token used for classification.\n",
    "\n",
    "When preparing text for input to DistilBERT, two key components are generated:\n",
    "- **`input_ids`**: These are the token IDs representing the input text. Each token is mapped to a unique identifier from the model's vocabulary.\n",
    "- **`attention_mask`**: This is a binary mask indicating which tokens should be attended to and which should be ignored (typically, padding tokens are ignored). It helps the model focus on the actual content of the text while disregarding any padding.\n",
    "\n",
    "After tokenization, the text is passed through DistilBERT to obtain embeddings. The output of DistilBERT includes embeddings for each token, but we are primarily interested in the embedding of the [CLS] token. This token is specially designated for representing the entire sequence, and its embedding serves as the sentence embedding.\n",
    "\n",
    "For our purposes, the sentence embedding corresponds to the embedding of the [CLS] token, which captures the overall meaning of the input sentence. This embedding can then be used in conjunction with image embeddings to perform various cross-modal tasks, such as matching or classification.\n",
    "\n",
    "By implementing the Text Encoder with DistilBERT and understanding the roles of tokenization, `input_ids`, `attention_mask`, and the CLS token, we ensure that textual descriptions are effectively transformed into embeddings that align with the image embeddings produced by the Image Encoder.\n",
    "\n",
    "Take some time to familiarize yourself with [Tokenizers](https://huggingface.co/docs/tokenizers/en/index) and [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 model_name: str = cfg.text_encoder,\n",
    "                 trainable: bool = False):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.tensor,\n",
    "                attention_mask: torch.tensor):\n",
    "        \n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_batch = next(iter(clip_dl))['caption']\n",
    "print(text_batch.keys())\n",
    "\n",
    "text_encoder = TextEncoder(cfg.text_encoder)\n",
    "text_enc_out = text_encoder(\n",
    "    input_ids=text_batch[\"input_ids\"],\n",
    "    attention_mask=text_batch[\"attention_mask\"]\n",
    ")\n",
    "\n",
    "print(text_enc_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Creating CLIP\n",
    "\n",
    "In the CLIP architecture, the model integrates both text and image encoders to produce embeddings in a shared feature space. Since the dimensionalities of text and image embeddings differ, projection heads are employed to map these embeddings into a common space, allowing for effective similarity computation.\n",
    "\n",
    "The `CLIPModel` class encompasses four primary components:\n",
    "1. **Text Encoder**: Converts text into embeddings.\n",
    "2. **Image Encoder**: Converts images into embeddings.\n",
    "3. **Text Projection Head**: Projects text embeddings into the shared feature space.\n",
    "4. **Image Projection Head**: Projects image embeddings into the same shared feature space.\n",
    "\n",
    "The projection heads ensure that both text and image embeddings are in a compatible format, making it possible to compare them directly.\n",
    "\n",
    "To streamline the training process, you can freeze the parameters of the image and text encoders. This approach focuses training on the projection heads, which are responsible for mapping the embeddings from their respective domains into the shared feature space. By freezing the encoders, you reduce computational overhead and concentrate on fine-tuning the projections, which are crucial for aligning text and image embeddings.\n",
    "\n",
    "To train CLIP effectively, the loss function requires a comparison of the embeddings in the shared space. Given that the goal is to align text and image embeddings such that their outer product approximates an identity matrix, the targets for the loss function are crucial. \n",
    "\n",
    "Since duplicates have been removed, you can use `torch.arange` to set the targets. This approach helps create a target matrix where the diagonal elements represent correct matches between text and image embeddings. This setup facilitates training the model to distinguish between correct and incorrect pairs by comparing the similarities of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim: int,\n",
    "                 proj_dim: int = cfg.projection_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(embedding_dim, proj_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(proj_dim, proj_dim)\n",
    "        self.layernorm = nn.LayerNorm(proj_dim)\n",
    "    \n",
    "    def forward(self, x: torch.tensor):\n",
    "        x = self.proj(x)\n",
    "        x = x + self.fc(self.gelu(x))\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 cfg: Config = cfg):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self,\n",
    "                x: dict):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Training CLIP\n",
    "\n",
    "### Training the CLIP Model\n",
    "\n",
    "Now that we have our `CLIPModel` set up with both text and image encoders, along with their respective projection heads, it’s time to train the model. The training process involves optimizing the alignment between text and image embeddings, so they can be effectively compared.\n",
    "\n",
    "Since the CLIP model consists of four distinct components (text encoder, image encoder, and their corresponding projection heads), it can be beneficial to use different learning rates for each component. This allows for finer control over the training process, ensuring that each part of the model is updated appropriately according to its role. This, alongside the training setup, has been done for you.\n",
    "\n",
    "Your task is to complete the training function and plot the training loss curve. This is not at all different from what you've done before.\n",
    "\n",
    "You are allowed to tweak parts of the training pipeline (including freezing/unfreezing the encoders, training for more epochs etc.) at your own risk - we just want to see a nice hockey-shaped loss curve that looks like it's converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_tensors_to_device(d: dict, device: torch.device) -> dict:\n",
    "    \"\"\"\n",
    "    Helper function for moving tensors inside (nested) dictionaries to a target device - not necessary to use but can be useful depending on implementation\n",
    "    \n",
    "    :param d: Dictionary with potential nested dictionaries and tensors.\n",
    "    :param device: The device to move tensors to (e.g., torch.device('cuda:0') or torch.device('cpu')).\n",
    "    :return: A new dictionary with tensors moved to the specified device.\n",
    "    \"\"\"\n",
    "    new_dict = {}\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            # Recursively process nested dictionaries\n",
    "            new_dict[k] = move_tensors_to_device(v, device)\n",
    "        elif isinstance(v, torch.Tensor):\n",
    "            # Move tensors to the device\n",
    "            new_dict[k] = v.to(device)\n",
    "        else:\n",
    "            # For non-tensor, non-dict items, just copy them as is\n",
    "            new_dict[k] = v\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel()\n",
    "params = [\n",
    "    {\"params\": model.image_encoder.parameters(), \"lr\": cfg.image_encoder_lr},\n",
    "    {\"params\": model.text_encoder.parameters(), \"lr\": cfg.text_encoder_lr},\n",
    "    {\"params\": itertools.chain(\n",
    "        model.image_projection.parameters(), model.text_projection.parameters()\n",
    "    ), \"lr\": cfg.head_lr, \"weight_decay\": cfg.weight_decay}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
    "epochs = 15\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss curve\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Zero-shot Image Classification\n",
    "\n",
    "Now for a very new type of downstream task.\n",
    "\n",
    "Zero-shot image classification is a technique where a model can correctly classify images into categories it has never seen during training. Unlike traditional classification models that require training on specific labeled examples for each category, a zero-shot model like CLIP leverages its ability to understand and relate images and text descriptions to classify new categories on the fly. This is possible because CLIP learns a joint embedding space for both images and text, allowing it to generalize to new tasks without additional training.\n",
    "\n",
    "CLIP achieves zero-shot classification by mapping both images and text (such as category names or descriptions) into the same embedding space. During inference, you provide the model with a set of textual descriptions of potential categories (e.g., \"a dog,\" \"a cat,\" \"a car\") and an image to classify. The model computes the similarity between the image embedding and each text embedding. The category with the highest similarity score is then chosen as the predicted label for the image.\n",
    "\n",
    "Here are the steps to perform it:\n",
    "\n",
    "1. **Prepare Your Text Prompts**: Write down the categories you want to classify the image into. For example, if you're classifying an animal, your categories might be \"a dog,\" \"a cat,\" \"a bird,\" etc. Note you can include full length sentences too, it's not limited to one-word categories anymore.\n",
    "\n",
    "2. **Encode the Text Prompts**: Use the text encoder part of the CLIP model to convert these category descriptions into embeddings.\n",
    "\n",
    "3. **Encode the Image**: Use the image encoder part of the CLIP model to convert the image into an embedding.\n",
    "\n",
    "4. **Compute Similarities**: Calculate the similarity between the image embedding and each of the text embeddings. This step determines how closely the image matches each category description.\n",
    "\n",
    "5. **Select the Best Match**: Identify the text prompt (category) that has the highest similarity score with the image. This category is your model's prediction.\n",
    "\n",
    "Your task is to create a function that performs the zero-shot classification as described. Once you've implemented the function, test it with an image of your choice.\n",
    "\n",
    "You may find the model does not perform spectacularly on some sets of images and prompts, but that's alright: we trained it on a very small dataset with a very small number of steps. You can look into scaling up in your own time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_classify(clip_model: nn.Module,\n",
    "                       tokenizer: Callable[[List[str]], Dict[str, torch.Tensor]],\n",
    "                       images: torch.Tensor,\n",
    "                       class_descriptions: List[str],\n",
    "                       transform: Optional[Callable[[Image.Image], Image.Image]] = None,\n",
    "                       device: str = \"cuda\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Perform zero-shot classification on images using textual class descriptions.\n",
    "\n",
    "    :param clip_model: PyTorch nn.Module class for the CLIP model\n",
    "    :param images: Tensor of images to classify.\n",
    "    :param class_descriptions: List of class descriptions (prompts) for zero-shot classification.\n",
    "    :return: Tensor of predicted class indices.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O dummy_img.jpg https://cdn11.bigcommerce.com/s-t04x4i8lh4/product_images/uploaded_images/how-to-exercise-your-dog-in-the-winter.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(\"dummy_img.jpg\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_classify(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    img,\n",
    "    [\"car driving through a city\", \"dog running through snow\", \"helicopter smashing into a tree\", \"students crying over an exam\"],\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((cfg.image_size, cfg.image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 0 Part 1 - Convolutional Neural Networks\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are a foundational model in deep learning, particularly for tasks involving visual data. Since their inception, CNNs have become central to computer vision, enabling advancements in areas like image classification, object detection, and segmentation. The key strength of CNNs lies in their ability to automatically learn hierarchical patterns in images, capturing features such as edges, textures, and complex structures as we progress through the layers.\n",
    "\n",
    "The history of CNNs dates back to the 1980s with the introduction of concepts like weight sharing and local connectivity, which set CNNs apart from traditional neural networks. However, it wasn’t until the early 2010s that CNNs gained widespread recognition, largely due to the success of the AlexNet model in the 2012 ImageNet competition. This breakthrough demonstrated the efficacy of deep CNNs in handling large-scale image datasets and catalyzed a wave of research and development in the field.\n",
    "\n",
    "In this assignment, we'll start by focusing on image classification—a classic application of CNNs. You will explore different types of convolutions, including depthwise separable convolutions, which reduce the computational cost by separating the spatial and channel-wise operations. We'll also delve into 1x1 convolutions, often used for dimensionality reduction, and bottleneck blocks, which are a combination of these techniques to make deep networks more efficient. Through these exercises, you'll gain a hands-on understanding of the various convolutional operations and how they contribute to building powerful and efficient CNN architectures.\n",
    "\n",
    "We will be using PyTorch for this assignment and the other two parts. Hence, it is recommended you brush up on your PyTorch before starting this assignment.\n",
    "\n",
    "Here is a list of resources to help you with this part:\n",
    "- [CS231n CNNs](https://cs231n.github.io/convolutional-networks/) - A great writeup on CNNs, both for beginners and people looking for a refresher.\n",
    "- [DeepLearningAI Intro to CNNs](https://youtube.com/playlist?list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&si=VI-V6EPFE-6DvAzE) - A free YouTube playlist to understand CNNs and other topics in Computer Vision.\n",
    "- [learnpytorch.io](https://www.learnpytorch.io/) - an online and free book for learning PyTorch. Take out a few hours to go through the first few chapters till you are familiar with Image Classification.\n",
    "- [PyTorch Official Intro to Image Classification](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) - If you want a faster primer, this will be helpful.\n",
    "- [Intro to Separable Convolutions](https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728) - A nice primer for Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_num_params(model: nn.Module):\n",
    "    '''Returns the number of parameters in a PyTorch model'''\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0. Getting our Data\n",
    "\n",
    "For this assignment, we'll be using the CIFAR10 dataset, a widely-used benchmark in computer vision that consists of 60,000 32x32 color images across 10 different classes. The dataset is conveniently available through the `torchvision` library, which provides a straightforward way to load and work with these images.\n",
    "\n",
    "We begin by loading CIFAR10 using `torchvision.datasets`, where we'll download the dataset and prepare it for training and testing. To improve the robustness of our models, we'll set up a series of augmentations. These augmentations might include random horizontal flips, random crops, or normalization, all designed to simulate variations that the model might encounter in real-world scenarios. This not only helps in preventing overfitting but also enhances the model's ability to generalize.\n",
    "\n",
    "Once the dataset and augmentations are defined, we'll utilize PyTorch’s `DataLoader` to manage the batching, shuffling, and efficient loading of the data during training. The `DataLoader` ensures that the model receives data in an optimized manner, which is crucial for speeding up the training process and maintaining model performance.\n",
    "\n",
    "Finally, it’s always beneficial to visualize your data before diving into model training. By plotting a few batches of images along with their corresponding labels, you can get a quick sense of the dataset and confirm that the augmentations are being applied correctly. This step is crucial for catching any issues early on and ensuring that your data pipeline is functioning as expected.\n",
    "\n",
    "You don't have to do anything for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the CIFAR-10 dataset\n",
    "root = \"./data\"\n",
    "\n",
    "tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "trainset = datasets.CIFAR10(\n",
    "    root, train=True, download=True, transform=tfms\n",
    ")\n",
    "testset = datasets.CIFAR10(\n",
    "    root, train=False, download=True, transform=tfms\n",
    ")\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch\n",
    "grid_img = torchvision.utils.make_grid(\n",
    "    tensor=next(iter(train_dl))[0], # grab a batch of images from the DataLoader\n",
    "    nrow=4\n",
    ")\n",
    "plt.imshow(grid_img.permute(1,2,0)) # move the channel axis to the end\n",
    "plt.axis(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Training a plain CNN from Scratch\n",
    "\n",
    "Now that we've set up our data pipeline, it's time to build your first Convolutional Neural Network (CNN). For this part of the assignment, you'll create a CNN architecture of your choice using basic building blocks provided by PyTorch: `nn.Conv2d`, `nn.MaxPool2d`, `nn.ReLU`, and `nn.Linear`. These components form the backbone of many classic CNN architectures, allowing you to understand the core principles without getting bogged down by complex layers or operations.\n",
    "\n",
    "One straightforward approach is to design a network similar to AlexNet or VGG-16. These architectures are characterized by alternating layers of convolution, max pooling, and activation functions. Specifically, you'll start by applying `nn.Conv2d` to extract features from the input images. This will be followed by `nn.MaxPool2d` to downsample the feature maps, reducing their spatial dimensions and computational load while retaining important information. The `nn.ReLU` activation function is then applied to introduce non-linearity, enabling the network to learn more complex patterns.\n",
    "\n",
    "As you stack these layers, you'll progressively increase the depth of the network, capturing increasingly abstract features. Finally, you'll flatten the feature maps and pass them through fully connected layers (`nn.Linear`) to perform classification.\n",
    "\n",
    "At this stage, there's no need to experiment with advanced techniques or fine-tuning. The goal is to become comfortable with constructing a basic CNN and understanding how each component contributes to the model's overall performance. In the subsequent sections, we'll explore more sophisticated techniques and architectures, but for now, focus on getting this foundational CNN up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNN from scratch\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_classes: int = 10):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your model\n",
    "model = CNN(num_classes=10)\n",
    "\n",
    "# Do a dummy forward pass on a batch from the DataLoader\n",
    "# This will help you debug shape errors\n",
    "x = next(iter(train_dl))[0]\n",
    "logits = model(x)\n",
    "print(logits.shape)\n",
    "\n",
    "print(f\"Model has {get_num_params(model)} parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the training process, we recommend reviewing a tutorial on image classification with PyTorch if you're not already familiar with it. This will give you a solid foundation in the key concepts and functions we'll be using.\n",
    "\n",
    "In the cell below, you'll train your CNN using the `nn.CrossEntropyLoss` as the loss function, which is standard for multi-class classification tasks. For optimization, we'll use the `Adam` optimizer with a learning rate of `3e-4`, a good starting point for training deep networks. You'll train the model for 10 epochs, which should be sufficient to observe significant learning without overfitting or excessive training time.\n",
    "\n",
    "If you find that your model is running slowly or consuming too much memory, consider revisiting your architecture. Simplifying the model or reducing its depth can speed up training and reduce the risk of running into issues, especially if you're working with limited resources.\n",
    "\n",
    "The target here is to achieve around 70% validation accuracy. This might seem modest, but remember that we're training the model from scratch, and this is just the beginning. Hitting this benchmark indicates that your model is learning and generalizing reasonably well.\n",
    "\n",
    "It's crucial to use a GPU for training, as it will dramatically speed up the process. Be cautious of device-related errors—ensure that your tensors and model are consistently placed on the GPU by using `.to(device)` where `device` is either `\"cuda\"` or `\"cpu\"` based on availability.\n",
    "\n",
    "To streamline your code, define functions for a single training step and a single validation step. This not only makes your code cleaner but also modular, meaning you can easily swap out or modify components in future experiments. You can refer to the relevant chapter in [learnpytorch.io](https://www.learnpytorch.io) for a detailed guide on structuring these functions. Packaging everything into functions is a best practice that allows you to experiment with different models, datasets, or hyperparameters without rewriting large portions of your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate loss function, optimizer, and device\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train_step and eval_step\n",
    "def train_step(model: nn.Module,\n",
    "               dataloader: DataLoader,\n",
    "               loss_fn: nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: str):\n",
    "    raise NotImplementedError\n",
    "\n",
    "@torch.inference_mode\n",
    "def eval_step(model: nn.Module,\n",
    "               dataloader: DataLoader,\n",
    "               loss_fn: nn.Module,\n",
    "               device: str):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined our model, loss function, optimizer, and training setup, it's time to actually run the training process. Using the functions you've defined earlier, you'll train the model on the CIFAR10 dataset over the specified number of epochs.\n",
    "\n",
    "As you train the model, it's important to monitor the training progress by printing out the loss and accuracy values after each epoch. This provides insight into how well the model is learning and whether adjustments might be needed. Make sure to save these values—both loss and accuracy for training and validation sets—so you can plot them later. These plots, often referred to as loss curves, will help you visualize the model's learning dynamics and diagnose any issues such as overfitting or underfitting.\n",
    "\n",
    "If you haven't already, make sure you're using a GPU to accelerate the training. Training deep networks on a CPU can be prohibitively slow, whereas a GPU can dramatically reduce the time required for each epoch. Before starting the training loop, ensure that both your model and data are correctly placed on the GPU by using `.to(device)`.\n",
    "\n",
    "As you run the training, keep an eye on the output for any potential issues, such as device mismatches or unusually high loss values, which could indicate problems in the data pipeline or model configuration. By the end of training, you should have a trained model with logged performance metrics, ready for evaluation and further experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for no more than 10 epochs (be sure to save the loss and accuracy values somewhere!)\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss curves\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Using 1x1 Convolutions to create Bottlenecks\n",
    "\n",
    "1x1 convolutions are a powerful tool in modern deep learning architectures, particularly when working with convolutional neural networks. Unlike traditional convolutions, which typically use larger kernels like 3x3 or 5x5, a 1x1 convolution applies a single filter across each pixel location, effectively operating on the depth dimension of the input tensor. This seemingly simple operation has been widely adopted in advanced architectures like InceptionNet and MobileNet due to its ability to reduce computational complexity while retaining valuable information.\n",
    "\n",
    "One of the primary uses of 1x1 convolutions is to downsample the number of channels in a tensor. For instance, imagine you have a tensor with a large number of channels after applying several convolutions. A 1x1 convolution can be used to reduce the channel dimension, significantly decreasing the computational overhead before applying more complex operations like a 3x3 convolution. After the 3x3 convolution, another 1x1 convolution can be applied to upsample the channels back to their original size or to a desired dimension. This three-stage approach—downsampling, applying the 3x3 convolution, and then upsampling—forms the basis of the bottleneck block, which is a cornerstone of many efficient deep learning models.\n",
    "\n",
    "To illustrate the efficiency of this method, let’s consider a small numerical example. Suppose you have an input tensor with 256 channels, and you want to apply a 3x3 convolution that outputs 512 channels. A direct 3x3 convolution would involve $256 \\times 512 \\times 3 \\times 3 = 1,179,648$ parameters. However, if you first downsample the 256 channels to 64 using a 1x1 convolution, apply the 3x3 convolution on the reduced tensor, and then upsample back to 512 channels with another 1x1 convolution, the total number of parameters becomes $256 \\times 64 \\times 1 \\times 1 + 64 \\times 512 \\times 3 \\times 3 + 512 \\times 256 \\times 1 \\times 1 = 16,384 + 294,912 + 131,072 = 442,368$ parameters. As you can see, this approach requires significantly fewer parameters than the direct method, even though it introduces additional layers. This not only reduces the model’s size but also makes it more efficient to train.\n",
    "\n",
    "Your next task will be to implement this type of architecture using bottleneck blocks, which were popularized by the ResNet architecture. ResNet, introduced in the paper [\"Deep Residual Learning for Image Recognition\"](https://arxiv.org/abs/1512.03385), leverages bottleneck blocks to allow the construction of very deep networks while maintaining computational efficiency. The residual connections in ResNet help in mitigating the vanishing gradient problem, making it feasible to train networks with hundreds or even thousands of layers.\n",
    "\n",
    "By implementing bottleneck blocks in your model, you’ll explore how deeper architectures can be both powerful and efficient, leveraging the combination of 1x1 convolutions and residual connections to push the boundaries of what CNNs can achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a tensor for demonstration\n",
    "in_channels = 64\n",
    "h, w = 224, 224\n",
    "bs = 8\n",
    "out_channels = 256\n",
    "\n",
    "x = torch.randn(bs, in_channels, h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A regular 3x3 Conv\n",
    "layer = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bottleneck using 1x1 Convs - note the same shape as above\n",
    "bottleneck_channels = in_channels // 4\n",
    "layer = nn.Sequential(\n",
    "    nn.Conv2d(in_channels, bottleneck_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "    nn.Conv2d(bottleneck_channels, bottleneck_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.Conv2d(bottleneck_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    ")\n",
    "\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions to help you tinker with comparing a single 3x3 conv against the three-stage approach\n",
    "def get_standard_conv(in_channels: int,\n",
    "                        out_channels: int):\n",
    "    \n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "def get_bottleneck_conv(in_channels: int,\n",
    "                        out_channels: int,\n",
    "                        bottleneck_channels: int):\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, bottleneck_channels, kernel_size=1, stride=1, padding=0),\n",
    "        nn.Conv2d(bottleneck_channels, bottleneck_channels, kernel_size=3, stride=1, padding=1),\n",
    "        nn.Conv2d(bottleneck_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "    )\n",
    "\n",
    "# Demonstrating how they have the same effect\n",
    "x = torch.randn(8, 64, 224, 224)\n",
    "out_1 = get_standard_conv(64, 32)(x)\n",
    "out_2 = get_bottleneck_conv(64, 32, bottleneck_channels=16)(x) # downsample by a factor of 4\n",
    "print(out_1.shape)\n",
    "print(out_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you'll explore the practical benefits of using bottleneck blocks by comparing them with a more straightforward CNN architecture. A simple implementation of a CNN, `PlainCNN`, using regular 3x3 convolutions, has been provided. Your task is to complete the forward pass of this model without modifying the constructor. This ensures that the architecture is consistent, allowing for a fair comparison.\n",
    "\n",
    "Next, you'll implement a `BottleneckCNN` model that mirrors the architecture of `PlainCNN`, but with a key difference: the standard convolutional blocks will be replaced by bottleneck blocks. These bottleneck blocks use 1x1 convolutions to downsample and upsample the channel dimensions, reducing the overall number of parameters while maintaining—or even improving—the network's capacity to learn complex features.\n",
    "\n",
    "Once both models are implemented, instantiate them and compare the number of parameters. You’ll likely find that the `BottleneckCNN` has significantly fewer parameters than `PlainCNN`, despite achieving a similar or better feature extraction capability. This reduction in parameters demonstrates how bottleneck blocks, with their use of 1x1 convolutions, can lead to more efficient models without sacrificing performance.\n",
    "\n",
    "This comparison not only reinforces the concept of efficient model design but also highlights the practical utility of architectural innovations like the bottleneck block, as popularized by the ResNet architecture. By experimenting with these models, you’ll gain a deeper understanding of how design choices impact both the size and efficiency of deep learning networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Initial Conv layer with kernel size of 7\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            \n",
    "            # Using get_standard_conv to build subsequent layers\n",
    "            get_standard_conv(64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            get_standard_conv(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            get_standard_conv(256, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "            \n",
    "        # Fully connected layer\n",
    "        self.head = nn.Linear(512 * 7 * 7, num_classes)  # Assuming input image size of 224x224\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Initialize model and do a dummy forward pass\n",
    "plain_cnn = PlainCNN(10)\n",
    "plain_cnn(next(iter(train_dl))[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "# Initialize model and do a dummy forward pass\n",
    "bottleneck_cnn = BottleneckCNN(10)\n",
    "bottleneck_cnn(next(iter(train_dl))[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the number of parameters across both\n",
    "print(f\"PlainCNN has {get_num_params(plain_cnn)} parameters.\")\n",
    "print(f\"BottleneckCNN has {get_num_params(bottleneck_cnn)} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the BottleneckCNN in the same fashion as before\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss curves\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Depthwise Separable Convolutions\n",
    "\n",
    "Depthwise Separable Convolutions are an advanced convolutional technique designed to improve the efficiency of convolutional neural networks by significantly reducing the number of parameters and computational cost. This approach decomposes the standard convolution operation into two distinct steps: depthwise convolution and pointwise convolution. By breaking down the convolution process, Depthwise Separable Convolutions offer a more efficient way to process visual data, making them a cornerstone in lightweight architectures like MobileNet.\n",
    "\n",
    "Depthwise Separable Convolutions have been integral to the development of efficient deep learning models, particularly in mobile and resource-constrained environments. MobileNet, a well-known architecture designed for mobile and embedded vision applications, relies heavily on Depthwise Separable Convolutions to achieve high accuracy with minimal computational resources. By incorporating these convolutions, MobileNet can run on devices with limited processing power, such as smartphones, while still delivering impressive performance on tasks like image classification and object detection.\n",
    "\n",
    "To understand Depthwise Separable Convolutions, it’s important to first grasp how they decompose a standard convolution. In a typical convolutional layer, a 3D kernel is applied across all channels of an input tensor simultaneously, mixing both the spatial and channel dimensions in a single operation. Depthwise Separable Convolutions separate this process into two distinct steps:\n",
    "\n",
    "1. **Depthwise Convolution**: In this step, each input channel is convolved independently using a separate 2D filter. This means that if the input tensor has `C` channels, `C` different filters are applied—one per channel. The result is that each channel is processed individually, preserving the spatial structure but not mixing information between channels. This operation reduces the computational cost because it avoids the full 3D convolution process, focusing only on the spatial dimension of each channel.\n",
    "\n",
    "2. **Pointwise Convolution**: After the depthwise convolution, a 1x1 convolution (referred to as pointwise) is applied across all the channels of the resulting tensor. This operation mixes the information across different channels, effectively combining the output of the depthwise convolution into a more meaningful representation. By only using 1x1 convolutions, pointwise convolution operates on the channel dimension, drastically reducing the number of parameters compared to a standard 3x3 or 5x5 convolution.\n",
    "\n",
    "**Depthwise Convolution**\n",
    "\n",
    "Depthwise convolution is the first stage of a Depthwise Separable Convolution. It involves applying a single convolutional filter to each input channel separately. For example, if the input tensor has dimensions `(H, W, C)`, where `H` is the height, `W` is the width, and `C` is the number of channels, the depthwise convolution would apply `C` separate filters to each channel, producing an output tensor of the same dimensions `(H, W, C)`.\n",
    "\n",
    "This operation primarily focuses on capturing spatial features within each channel but does not mix information between different channels. Because it avoids cross-channel operations, the depthwise convolution is computationally efficient, significantly reducing the number of multiplication operations required.\n",
    "\n",
    "**Pointwise Convolution**\n",
    "\n",
    "After the depthwise convolution has processed each channel independently, pointwise convolution comes into play. This is simply a 1x1 convolution applied across the output channels of the depthwise convolution. Despite its simplicity, the pointwise convolution is crucial because it enables the network to combine information across different channels, effectively reintroducing the depth that was initially separated.\n",
    "\n",
    "In a pointwise convolution, every output pixel is a weighted sum of the corresponding input pixels across all channels, allowing the model to learn complex combinations of features extracted by the depthwise convolution. This step significantly increases the representational power of the model while maintaining a low computational overhead compared to traditional convolutions.\n",
    "\n",
    "**Implementing Depthwise Separable Convolutions in a Model**\n",
    "\n",
    "Your next task is to implement a model that utilizes Depthwise Separable Convolutions. By replacing standard convolutions with depthwise and pointwise convolutions, you'll observe how this technique can lead to a more parameter-efficient model without compromising on accuracy. This exercise will deepen your understanding of how modern CNN architectures are optimized for performance, particularly in scenarios where computational resources are limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "\n",
    "        raise NotImplementedError\n",
    "    \n",
    "        # Depthwise convolution\n",
    "        self.depthwise = None\n",
    "        # Pointwise convolution\n",
    "        self.pointwise = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "    \n",
    "class DepthwiseSeparableCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "# Initialize model and do a dummy forward pass\n",
    "depthwise_sep_cnn = DepthwiseSeparableCNN(10)\n",
    "depthwise_sep_cnn(next(iter(train_dl))[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"DepthwiseSeparableCNN has {get_num_params(depthwise_sep_cnn)} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the DepthwiseSeparableCNN in the same fashion as before\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the loss curves\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
